{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于transformer中decoder-only架构的LLM模型 为什么建议左填充的case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel ,BertTokenizer\n",
    "from transformers import LlamaForCausalLM ,LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构造两个不等长的输入句子\n",
    "input_text = [\n",
    "    \"I want to go to space\",\n",
    "    \"I'm going to Greece for my holiday to see the beauty\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 之前 encoder-only 模型的填充一般是right-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    }
   ],
   "source": [
    "## 之前 encoder-only 模型一般都是右填充\n",
    "bert_tokenizer= BertTokenizer.from_pretrained(\"./link_model/bert-base-uncased/\")\n",
    "bert = BertModel.from_pretrained(\"./link_model/bert-base-uncased/\")\n",
    "print(bert_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1045, 2215, 2000, 2175, 2000, 2686,  102,    0,    0,    0,    0,\n",
      "           0,    0,    0])\n",
      "tensor([ 101, 1045, 1005, 1049, 2183, 2000, 5483, 2005, 2026, 6209, 2000, 2156,\n",
      "        1996, 5053,  102])\n"
     ]
    }
   ],
   "source": [
    "## bert tokenzier 之后\n",
    "tokens = bert_tokenizer(input_text,padding=\"longest\",return_tensors=\"pt\")\n",
    "print(tokens.input_ids[0])\n",
    "print(tokens.input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1180,  0.3472,  0.0159,  ..., -0.1407,  0.3505,  0.3096],\n",
       "          [ 0.2318,  0.4219,  0.1071,  ..., -0.0161,  0.5884,  0.1275],\n",
       "          [ 0.3912,  0.2970,  1.1053,  ...,  0.0593, -0.0361, -0.0474],\n",
       "          ...,\n",
       "          [ 0.0150,  0.0878,  0.1920,  ...,  0.2756,  0.0966,  0.0368],\n",
       "          [ 0.3867,  0.3596,  0.2187,  ...,  0.1566, -0.0414,  0.1783],\n",
       "          [ 0.2775,  0.3966,  0.1957,  ...,  0.1723, -0.0351,  0.2319]],\n",
       " \n",
       "         [[ 0.1144,  0.1349, -0.0943,  ..., -0.3948,  0.2984,  0.3757],\n",
       "          [ 0.3793, -0.0288,  0.0120,  ..., -0.2841,  0.3794,  0.4573],\n",
       "          [ 0.6562,  0.2536, -0.0305,  ..., -0.4044, -0.7741, -0.2387],\n",
       "          ...,\n",
       "          [ 0.0656, -0.2254,  0.5395,  ..., -0.5768,  0.1196, -0.1314],\n",
       "          [ 0.2855, -0.5172,  0.2094,  ...,  0.0477,  0.3329, -0.1484],\n",
       "          [ 0.6014,  0.2732,  0.0175,  ..., -0.1611, -0.7703, -0.3335]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-0.8469, -0.2697,  0.1834,  ...,  0.2944, -0.6262,  0.8889],\n",
       "         [-0.8363, -0.3810, -0.2348,  ..., -0.0139, -0.6042,  0.9215]],\n",
       "        grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(input_ids= tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"],return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### decoder-only 使用右填充会出现的问题,以及得到的警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llama_tokenzier = LlamaTokenizer.from_pretrained(\"./link_model/llama2-7b-hf/\")\n",
    "llama_model = LlamaForCausalLM.from_pretrained(\"./link_model/llama2-7b-hf/\",trust_remote_code= True)\n",
    "llama_tokenzier.pad_token = llama_tokenzier.eos_token\n",
    "print(llama_tokenzier.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,  306,  864,  304,  748,  304, 2913,    2,    2,    2,    2,    2,\n",
      "           2,    2,    2])\n",
      "tensor([    1,   306, 29915, 29885,  2675,   304, 25549,   363,   590,  8753,\n",
      "        22394,   304,  1074,   278, 15409])\n"
     ]
    }
   ],
   "source": [
    "## 使用llama tokenzier进行token 化\n",
    "tokens = llama_tokenzier(input_text,padding=\"longest\",return_tensors=\"pt\")\n",
    "print(tokens.input_ids[0])\n",
    "print(tokens.input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/yyb-finetune/lib/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I want to go to space nobody wants to go',\n",
       " \"I'm going to Greece for my holiday to see the beauty of the country and to\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llama_model.generate(pad_token_id=llama_tokenzier.pad_token_id, **tokens)\n",
    "res_text = llama_tokenzier.batch_decode(output,skip_special_tokens=True)\n",
    "res_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用left-padding 可以消除上述的警告,以及left-padding的优势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "tensor([   2,    2,    2,    2,    2,    2,    2,    2,    1,  306,  864,  304,\n",
      "         748,  304, 2913])\n",
      "tensor([    1,   306, 29915, 29885,  2675,   304, 25549,   363,   590,  8753,\n",
      "        22394,   304,  1074,   278, 15409])\n"
     ]
    }
   ],
   "source": [
    "llama_tokenzier = LlamaTokenizer.from_pretrained(\"./link_model/llama2-7b-hf/\",padding_side = \"left\")\n",
    "llama_tokenzier.pad_token = llama_tokenzier.eos_token\n",
    "print(llama_tokenzier.padding_side)\n",
    "\n",
    "tokens = llama_tokenzier(input_text,padding=\"longest\",return_tensors=\"pt\")\n",
    "print(tokens.input_ids[0])\n",
    "print(tokens.input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s></s></s></s></s></s></s></s><s>I want to go to space. I want to go',\n",
       " \"<s>I'm going to Greece for my holiday to see the beauty of the country and to\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llama_model.generate(pad_token_id=llama_tokenzier.pad_token_id, **tokens)\n",
    "res_text = llama_tokenzier.batch_decode(output)\n",
    "res_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yyb-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
