{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/yyb-finetune/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama2_tokenzier_naive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<0xE5>', '<0x9E>', '<0x83>', '<0xE5>', '<0x9C>', '<0xBE>', '分', '类', '，', '一', '<0xE8>', '<0x88>', '<0xAC>', '是', '指', '按', '一', '定', '<0xE8>', '<0xA7>', '<0x84>', '定', '或', '标', '<0xE5>', '<0x87>', '<0x86>', '将', '<0xE5>', '<0x9E>', '<0x83>', '<0xE5>', '<0x9C>', '<0xBE>', '分', '类', '<0xE5>', '<0x82>', '<0xA8>', '存', '、', '<0xE6>', '<0x8A>', '<0x95>', '放', '和', '<0xE6>', '<0x90>', '<0xAC>', '运', '，', '从', '而', '转', '变', '成', '公', '共', '<0xE8>', '<0xB5>', '<0x84>', '源', '的', '一', '系', '列', '活', '动', '的', '<0xE6>', '<0x80>', '<0xBB>', '称', '。']\n",
      "[1, 29871, 232, 161, 134, 232, 159, 193, 30748, 30832, 30214, 30287, 235, 139, 175, 30392, 31084, 31590, 30287, 30495, 235, 170, 135, 30495, 31391, 31062, 232, 138, 137, 30998, 232, 161, 134, 232, 159, 193, 30748, 30832, 232, 133, 171, 30946, 30330, 233, 141, 152, 31182, 30503, 233, 147, 175, 31894, 30214, 31594, 31325, 31415, 31462, 30494, 30539, 31611, 235, 184, 135, 31193, 30210, 30287, 31185, 31025, 31704, 30846, 30210, 233, 131, 190, 31685, 30267]\n",
      "id_len: 76\n",
      "<s> 垃圾分类，一般是指按一定规定或标准将垃圾分类储存、投放和搬运，从而转变成公共资源的一系列活动的总称。\n"
     ]
    }
   ],
   "source": [
    "## 初始tokenzier 分词结果\n",
    "text = '垃圾分类，一般是指按一定规定或标准将垃圾分类储存、投放和搬运，从而转变成公共资源的一系列活动的总称。'\n",
    "print(tokenizer.tokenize(text))  \n",
    "id = tokenizer.encode(text)\n",
    "print(id)\n",
    "print(\"id_len:\",len(id))\n",
    "print(tokenizer.decode(id))  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39888\n",
      "{'id': 1, 'uniqueKey': 'da73e2d0bb4e39d241c3806876621da7', 'titleUkey': 'da73e2d0bb4e39d241c3806876621da7', 'dataType': '博客', 'title': '引网站蜘蛛的方法', 'content': '做站长的都希望自已做的网站被搜索引擎比如百度尽早收录。对于新站来说,蜘蛛可没不是呼之即来挥之即去的。 但是也不是一筹莫展,无计可施,只有摸透了这一只只神秘莫测的蜘蛛,有的放矢,才能随心所欲,对吧。呵呵。些话不多说,言归正传吧。 第一,蜘蛛的出动其实是非常讲究效率的,他们也懒得白跑,如果你的网站十天半个月不更新,他白跑几次后,也就不会来这么勤了。 所以,为了让蜘蛛天天来,那么就务必不让他空来,每次都喂点食。所以对策说就是最好每天更新内容了。 可以说,你规律性的多久更新一次,蜘蛛很可能也多久才来一次。 第二,尽量去掉网页上可有可无的部分吧,特别是java之类的,还有过大的图片,要尽量降低网页加载负荷,,加速网页的打开速度,网页打速度快,那么用户体验才好,跳出率才低,网页评分才高。 第三,检查内部链接结构,去除死链接和重复链接；死链接让蜘蛛原地打转,重复连接降低网页的新鲜度； 第四,尽量多从正规和相关站点获得反向链接,正规的链接能确保外链的稳定,以及免收株连；相关链接能提高外链的权重； 第五,为站点制作网站地图,包括格式和xml两种格式,作为蜘蛛爬行的向导,让蜘蛛能爬满整个网站而没有遗漏； 第六,确保服务器返回正确的数据包响应,这条比较玄,还不懂,你可以跟我说下什么意思； 第七、为每个页面制作独立的标题和meta标签(关键字、描述),这个在网页模板里写好调用代码就行； 第八、查看网页日志,监测蜘蛛的的爬行记录,蜘蛛爬行后会留下足迹,查看这些足迹就知道蜘蛛什么时候曾经光顾过这里了。 第九,直接快速的方法:用繁殖池自动繁殖引蜘蛛,它可以快速收录站群链接或者外推链接,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以增加网站收录,提升网站排名。使用繁殖池引蜘蛛到其他平台发布外链不会受到种种限制。 需要繁殖池的联系官方qq: 咨询,马上为您申请开通！'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"./data/wudao/wudao_simple_sample.json\",\"r\") as file :\n",
    "    data1 = json.load(file)\n",
    "print(len(data1))   \n",
    "print(data1[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wudao_sample.json', 'wudao_simple_sample.json']\n",
      "./data/wudao/wudao_simple_sample.json\n",
      "wudao数据集大小为39888\n",
      "\n",
      " 0 引网站蜘蛛的方法 做站长的都希望自已做的网站被搜索引擎比如百度尽早收录。对于新站来说,蜘蛛可没不是呼之即来挥之即去的。 但是也不是一筹莫展,无计可施,只有摸透了这一只只神秘莫测的蜘蛛,有的放矢,才能随心所欲,对吧。呵呵。些话不多说,言归正传吧。 第一,蜘蛛的出动其实是非常讲究效率的,他们也懒得白跑,如果你的网站十天半个月不更新,他白跑几次后,也就不会来这么勤了。 所以,为了让蜘蛛天天来,那么就务必不让他空来,每次都喂点食。所以对策说就是最好每天更新内容了。 可以说,你规律性的多久更新一次,蜘蛛很可能也多久才来一次。 第二,尽量去掉网页上可有可无的部分吧,特别是java之类的,还有过大的图片,要尽量降低网页加载负荷,,加速网页的打开速度,网页打速度快,那么用户体验才好,跳出率才低,网页评分才高。 第三,检查内部链接结构,去除死链接和重复链接；死链接让蜘蛛原地打转,重复连接降低网页的新鲜度； 第四,尽量多从正规和相关站点获得反向链接,正规的链接能确保外链的稳定,以及免收株连；相关链接能提高外链的权重； 第五,为站点制作网站地图,包括格式和xml两种格式,作为蜘蛛爬行的向导,让蜘蛛能爬满整个网站而没有遗漏； 第六,确保服务器返回正确的数据包响应,这条比较玄,还不懂,你可以跟我说下什么意思； 第七、为每个页面制作独立的标题和meta标签(关键字、描述),这个在网页模板里写好调用代码就行； 第八、查看网页日志,监测蜘蛛的的爬行记录,蜘蛛爬行后会留下足迹,查看这些足迹就知道蜘蛛什么时候曾经光顾过这里了。 第九,直接快速的方法:用繁殖池自动繁殖引蜘蛛,它可以快速收录站群链接或者外推链接,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以增加网站收录,提升网站排名。使用繁殖池引蜘蛛到其他平台发布外链不会受到种种限制。 需要繁殖池的联系官方qq: 咨询,马上为您申请开通！\n",
      "\n",
      "\n",
      " 1 千站云繁殖池收录神器是什么? 千站云繁殖池是一款全新的升级收录算法,全面升级,6大功能,快速收录站群链接或者外推链接,已经取代于蜘蛛池,蜘蛛池的效果差已过时了； 那什么是繁殖池？适用于什么？有什么作用？ 繁殖池是大量网站将百度蜘蛛来访时集成一个池,通过程序控制自动繁殖外链地址给蜘蛛,这样可以快速大量收录站群链接或者外推链接； 适用于医疗媒体外推、站群、泛站、目录群、寄生虫、博客、微博、论坛、b2b信息,全自动繁殖不同地址引蜘蛛,实用而操作简单,效果好,可以实现亿万级蜘蛛网互联互串引蜘蛛,可以让新站、外推链接、媒体链接等等快速增加收录,被k网站也可以尽快恢复权重和搜索引擎快照,正常收录的网站可以增加网站收录,提升网站排名让你感受到不再需要为引蜘蛛到其他平台发布外链而受到种种限制等欢喜； 如果需要可以联系官方qq: 马上申请为你开通！！！\n",
      "\n",
      "\n",
      " 2 seo是什么?为什么要学习seo? 也许很多人对seo有迷惑,所以这篇文章主要是写给一些seo行业外的朋友们。对于seo是什么？学习seo的目的是什么？很多朋友因为不懂这个行业,认为不就是一门职业吗？只不过是365行其中一个行业而已。没错,seo就是一门职业,而且学会了seo发展空间是非常大的。为什么我说seo有前途呢？下面我将详细给朋友们探讨一下为什么要学习seo,好处有哪些 所谓seo什么,简单的说就是一个产品通过百度或者其他搜索引擎浏览器,输入某个关键词能出现在百度首页。做到这样的网络推广效果的技术叫做seo,也叫百度关键词排名技术。随着互联网时代快速的发展,传统的企业生意是一天不如一天,客户量和业务量越来越少,很多企业在走投无路的情况下,依然选择了网络这块大市场,希望通过网络能解决客户和业务问题。没错seo确实能给一些企业带来源源不断的客户量,而且做这个完全是不需要付费,可以说只要有技术就可以把流量做起来。因此seo人员也成为了众多企业眼中的财神爷。也许我这么说,朋友们听了会冷笑,不就是一门职业吗？至于那么夸大其词吗？没事,因为你不了解seo真正的魅力,没办法理解也是正常的哈。 那为什么要学习seo？是因为seo技术我个人觉得值得去专注学习,甚至能让我所有的业余时间都能放在这个上面,除了吃饭和睡觉,没错,在我眼里的百度优化关键词排名技术一定能圆满我人生的理想。 学习学精seo你将会有3种职业发展方向: 第一种职业方向:对于一些想往职业生涯上发展的朋友,而苦于找不到一门好的工作,seo优化推广技术能满足你,在杭州seo工作的朋友,月入过万的人大有人在,只要你用心学习seo,哪怕你什么都不懂是一张白纸也没关系,去专业的培训机构学习几个月下来,你就可以找到一份月收入至少5000的工作。 第二种职业方向:你可以通过系统的seo培训之后,搭建一个地区seo网站,比如:杭州seo。用你学习的网站优化技术,优化相关关键词做到百度首页,从而让你能够在家也能接单赚钱。这个职业发展方向,已经是属于自由职业者,等于自己做老板一样。 第三种职业方向:那就是自己创业,通过自己精通的网站优化技术,建立属于自己的网络公司,为很多企业提供网络推广服务,像seo创业,你完全是零投资纯赚钱的,唯一的投资就是前期学习seo,当你学会以后,可以通过网站来盈利实现被动受益,随着你的业务量多起来,你也可以找一些专门做seo优化的公司,价格谈好,把一些单子外包出去,这样合作就能在这个行业赚取非常多的财富。 以上就是谈谈为什么要学习seo的原因。seo是一个非常有前途的行业,今天花了点时间简单的给大家分享了一下什么是seo,这篇文章主要是写给一些不了解seo的朋友,不管你是一个公司的员工,还是一家公司的老板,你都该好好看一下,选择大于努力,什么样的想法就会有什么样的结果！有时间可以大家互相交流学习一下。 再跟大家聊聊分享一个如何快速收录网站引蜘蛛的方法。可以使用千站云繁殖池,千站云繁殖池是蜘蛛池的升级版,是大量网站将百度蜘蛛来访时集成一个池,通过程序控制自动繁殖外链地址给蜘蛛,这样可以快速大量收录站群链接或者外推链接。 繁殖池是一款全新的升级收录算法神器,按规则自动生成动态地址引蜘蛛,无需提交链接,只需要设置地址规则 即可。支持医疗推广,媒体外推、站群、泛站、目录群、寄生虫、博客、微博、论坛、b2b信息等；使用繁殖池可以让新站、外推链接、媒体链接等等快速增加收录,被k网站可以尽快恢复权重和搜索引擎快照,正常收录的网站可以增加网站收录,提升网站排名。详细咨询可以添加qq: ,也可以马上为您申请开通！！！\n",
      "\n",
      "\n",
      " 3 如何挖掘网站的\"卖点\"和怎么引百度蜘蛛 中国seo:如何挖掘网站的\"卖点\",以及怎么引百度蜘蛛 做一个优秀的seo,该如何做好自已的职责呢？传统意义中的卖点是指商品具备了前所未有、别出心裁或与众不同的特色、特点。这些特点、特色,一方面是产品与生俱来的,另一方面是通过营销策划人的想像力、创造力来产生\"无中生有\"的。而在网络推广中我把\"卖点\"一词理解为一个网站所独有的\"宣传点\"。如php博客以\"零\"为网站的宣传点,这样的宣传会激起那些没有进入而正准备进入这个行业的朋友极大的兴趣。网站卖点的挖掘,是网络推广的起步阶段,也是网络推广的最重要阶段。如何成功挖掘一个网站的卖点,对于网站的后期推广是至关重要的。那么如何成功挖掘一个网站的宣传点呢？以及收录百度蜘蛛呢？千站云繁殖池a便认为主要应做好以下几点: 一、发掘网站的卖点: 1、找准目标用户群 你的网站是针对哪个用户群？覆盖面有多广？这些是必须在挖掘卖点前需要搞清的因素！你是针对青少年还是老年人,你是针对男性还是女性做宣传(这是人群范围),你是做地区宣传还是全国宣传(这是地域范围),你是针对地产行业还是金融行业做宣传(这是行业范围)...等等！这些是你在挖掘一个网站卖点前需要搞清楚的,因为网站的卖点是可以创造的,而需求卖点是既定的！ 2、学会为网站创造卖点 找准用户群,找准目标客户后,我们就要为目标网站创造一个可以吸引目标客户的卖点。卖点必须是独有的,而且吸引人的。卖点的创造需要分析目标用户的生活习惯、爱好、地域等等方面的因素。在这方面国内用很多大网站提供了很好的参考实例,我认为中国的互联网就是一场模仿秀,别人有了google我们就有了baid ,有了icq后我们就有了qq,就是最近火爆的团购也是模仿秀的产物。但是让人惊奇的是,这些模仿者却无一例外的在中国取的了成功,而那些被模仿者却无一例外的遭到了惨痛的失败。 3、卖点需不断推陈出新 卖点不是恒久不变的,卖点也有保质期。任何一个网站需要在不同的时间段、不同的环境下推出自己的卖点。天热了避暑就是卖点,天凉了取暖就是卖点。打仗要换战术,网络推广要换策略,老是一种手段打仗会吃败仗的。那么做好这一点我认为应多观察外部环境,多观察时事,多了解目标用户的动态。只有及时了解这些信息,了解目标用户需要什么才能在卖点上不断的推陈出新。 二、 那如何引百度蜘蛛呢？现在百度越来越严格,新站收录的问题一直以来困扰很多站长,因为搜索引擎本身的不完美导致了很多新站在上线后不能被及时收录,有的新站上线半年以后才被收录,有的网站也许还不会有出道之时！这个方法可以很好的解决收录问题！只要执行力做好了再难收录的网站也会在几天之内被收录！这里要强调的是一定要是正规的站点！ 快速收录的方法,可以使用千战云繁殖池收录神器。繁殖池是蜘蛛池的升级版,是大量网站将百度蜘蛛来访时集成一个池,通过程序控制自动繁殖外链地址给蜘蛛,这样可以快速大量收录站群链接或者外推链接。 千站云繁殖池支持医疗推广、媒体外推、站群、泛站、目录群、寄生虫、博客、微博、论坛、b2b信息,可以快速引蜘蛛大量收录。可以让新站、外推链接、媒体链接等等快速增加收录,被k网站可以尽快恢复权重和搜索引擎快照,正常收录的网站可以增加网站收录,提升网站排名等。详细了解可以咨询千站云繁殖池的官方qq: 咨询！ 最后分享我最喜欢的一句教语:一个勇气可嘉的人我们称为勇士 ,一个善于计谋的人我们称之为谋士。而一个有勇有谋的人可以当将军,一个无勇无谋的人充其量也只能当一个士兵。大家每天都需要进步一点。\n",
      "\n",
      "\n",
      " 4 探讨做如何快速收录网站? 网站迟迟不收录怎么办呢？使用千站云繁殖池可以帮你搞定,让你不再需要为引蜘蛛到其他平台发布外链而受到种种限制。繁殖池快速收录站群链接或者外推链接,全自动繁殖不同地址引蜘蛛,实现亿万级蜘蛛网互联互串引蜘蛛。这样就可以解决网站不收录的问题,下面我将给大家介绍一下怎么使用繁殖池,审核申请开通池后,客服将会提供繁殖池的后台给自已操作(不需要下载),登陆账户就可以使用,然后添加网址进行引蜘蛛就可以了,如下图: 1、使用后记得要时常检查网站的外链,高质量的外链会给网长带来很大的好处。(千站云繁殖池是蜘蛛池的升级版,轮链繁殖池(每一个池可以放10万链接) 2、查一下网站是不是优化过度了,如果网站优化,过度蜘蛛是会对网站进行屏蔽的。(目录繁殖池(可日租周租月租),联系官方qq: 3、网站设计的结构不合理,如果网站的页面独立、互相没有关联也会引致收录不理想的。(地址繁殖池(有百度、360、搜狗,都是独立池) 4、要记得时常检查一下你网站上的内容是不是存在一些敏感的词,涉及到一些敏感的问题搜索引擎是不会收录的。(参数繁殖池(我们引的都是有效蜘蛛) 5、要时常分析网站内容。如果网站上的内容都是采集来的,或者是在其他网站上摘抄的类似内容,蜘蛛往往是不收录的。(泛站繁殖池(不限制域名) 6、记得要检查服务器或者空间的稳定性,还要检查你的网站程序里是否有不当的代码。这些都会对你的网站形成不好的影响的。(pc与移动是同步的) 7、要记得检查一下网站的robots.txt 文件,查一下它是否屏蔽了百度蜘蛛。这一步虽然简单,却也是很多站长容易忽略的步骤。(只需添加主站目录即可自动生成链接引蜘蛛,支持动态静态链接) 8、不论是什么网站,只要是新建的网站不管是哪个搜索引擎都不会贸然收录的,都会先观察一段时间的。这段时间蜘蛛可能不会做任何收录,站长也不要着急。只要继续更新高质量的内容蜘蛛肯定会收录的。此时不要对网站做大的改动,尤其是网站的结构。如果哪们站长的新网站不收录或者是收录不理想可以对照以上几个方面进行分析,相信你肯定能解决问题的方法的。 详细了解可以联系官方qq: 咨询！！！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## step 0 process_text\n",
    "data_dir_path = \"./data/wudao\"\n",
    "def process_wudao_sample():\n",
    "    \n",
    "    file_list = os.listdir(data_dir_path)\n",
    "    print(file_list)\n",
    "    wudao_data = open(f'{data_dir_path}/../all_data.txt', 'w', encoding='utf-8')\n",
    "    cnt=0\n",
    "    for file in file_list[1:]:\n",
    "        print(f'{data_dir_path}/{file}')\n",
    "        with open(f'{data_dir_path}/{file}','r',encoding='utf-8') as f:\n",
    "                data=json.load(f)\n",
    "                for item in data:\n",
    "                    wudao_data.write(item['title']+' '+item['content']+'\\n')\n",
    "                    cnt += 1\n",
    "    print(f\"wudao数据集大小为{cnt}\")\n",
    "    wudao_data.close()\n",
    "process_wudao_sample()\n",
    "\n",
    "### 查看部分数据\n",
    "\n",
    "### 查看部分数据并采样1000条\n",
    "data_path = \"./data/all_data.txt\"\n",
    "with open(f'{data_path}', 'r', encoding='utf-8') as f:\n",
    "    with open(f'./data/all_data_sample_10000.txt', 'w', encoding='utf-8') as fw:\n",
    "     for idx, line in enumerate( f.readlines()):\n",
    "            if(idx<10000):\n",
    "                fw.write(line)\n",
    "            if(idx< 5):\n",
    "                print(\"\\n\",idx,line)\n",
    "     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sentencespice 知识链接](https://zhuanlan.zhihu.com/p/630696264ncespice)\n",
    "\n",
    "https://www.cnblogs.com/hypnus-ly/p/15311847.html\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/630696264\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time\\nimport sentencepiece as sp\\n\\nstart_time = time.time()\\nprint(\"start\", start_time)\\nsp.SentencePieceTrainer.train(\\n    input=\"./data/all_data_sample_10000.txt\",  # 输入文件\\n    model_prefix=\"y-gpt-18k-1\",  # 模型前缀\\n    shuffle_input_sentence=False,  # 是否打乱句子\\n    train_extremely_large_corpus=True,\\n    # hyperparameters of tokenizer\\n    max_sentence_length=16384,  # 句子最大长度\\n    pad_id=3,\\n    model_type=\"BPE\",\\n    vocab_size=18000,\\n    split_digits=True,\\n    split_by_unicode_script=True,\\n    byte_fallback=True,\\n    allow_whitespace_only_pieces=True,\\n    remove_extra_whitespaces=False,\\n    normalization_rule_name=\"nfkc\",\\n)\\nend_time = time.time()\\nprint(end_time - start_time)\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 1 训练自己的分词器 使用和llama一致的sentencepiece  jupyter中有点问题 换到python文件可以运行\n",
    "'''\n",
    "import time\n",
    "import sentencepiece as sp\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"start\", start_time)\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input=\"./data/all_data_sample_10000.txt\",  # 输入文件\n",
    "    model_prefix=\"y-gpt-18k-1\",  # 模型前缀\n",
    "    shuffle_input_sentence=False,  # 是否打乱句子\n",
    "    train_extremely_large_corpus=True,\n",
    "    # hyperparameters of tokenizer\n",
    "    max_sentence_length=16384,  # 句子最大长度\n",
    "    pad_id=3,\n",
    "    model_type=\"BPE\",\n",
    "    vocab_size=18000,\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    byte_fallback=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    remove_extra_whitespaces=False,\n",
    "    normalization_rule_name=\"nfkc\",\n",
    ")\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '垃', '圾', '分', '类', ',', '一般', '是指', '按', '一定', '规定', '或', '标准', '将', '垃', '圾', '分', '类', '储', '存', '、', '投', '放', '和', '搬', '运', ',', '从而', '转', '变成', '公', '共', '资源', '的一', '系列', '活动', '的', '总', '称', '。']\n",
      "[3003, 5284, 5292, 3077, 3432, 3001, 343, 1031, 3478, 300, 286, 3121, 481, 3211, 5284, 5292, 3077, 3432, 4680, 3424, 3005, 3582, 3255, 3035, 4985, 3411, 3001, 795, 3291, 1382, 3119, 3462, 890, 425, 1311, 495, 3002, 3412, 3503, 3004]\n",
      "['\\n垃圾分类,一般是指按一定规定或标准将垃圾分类储存、投放和搬运,从而转变成公共资源的一系列活动的总称。\\n']\n",
      "['垃圾分类,一般是指按一定规定或标准将垃圾分类储存、投放和搬运,从而转变成公共资源的一系列活动的总称。']\n",
      "token_len: 40\n"
     ]
    }
   ],
   "source": [
    "## step 2 测试一下分词效果 1\n",
    "import sentencepiece as spm\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('y-gpt.model')\n",
    "\n",
    "text = '垃圾分类，一般是指按一定规定或标准将垃圾分类储存、投放和搬运，从而转变成公共资源的一系列活动的总称。'\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(text))\n",
    "print(sp.encode_as_ids(text))\n",
    "\n",
    "print(sp.decode_pieces([['▁', '<0x0A>', '垃圾', '分类', ',', '一般', '是指', '按', '一定', '规定', '或', '标准', '将', '垃圾', '分类', '储存', '、', '投放', '和', '搬运', ',', '从而', '转变成', '公共', '资源', '的一系列', '活动', '的总称', '。', '<0x0A>']]))\n",
    "print(sp.decode_ids([sp.encode_as_ids(text)]))\n",
    "print(\"token_len:\",len(sp.encode_as_ids(text)))\n",
    "\n",
    "### 可以发现新的tokenzier 将converted_id len 从 70+ ==> 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '垃圾', '分类', ',', '一般', '是指', '按', '一定', '规定', '或', '标准', '将', '垃圾', '分类', '储存', '、', '投', '放', '和', '搬', '运', ',', '从而', '转', '变成', '公共', '资源', '的一', '系列', '活动的', '总', '称', '。']\n",
      "[13156, 3441, 4100, 13154, 342, 954, 13579, 308, 271, 13262, 428, 13375, 3441, 4100, 7685, 13158, 13680, 13442, 13188, 15172, 13546, 13154, 835, 13404, 1596, 1796, 1029, 434, 1154, 5021, 13584, 13677, 13157]\n",
      "['\\n垃圾分类,一般是指按一定规定或标准将垃圾分类储存、投放和搬运,从而转变成公共资源的一系列活动的总称。\\n']\n",
      "['垃圾分类,一般是指按一定规定或标准将垃圾分类储存、投放和搬运,从而转变成公共资源的一系列活动的总称。']\n",
      "token_len: 33\n"
     ]
    }
   ],
   "source": [
    "## step 2 测试一下分词效果 1\n",
    "import sentencepiece as spm\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('y-gpt-18k.model')\n",
    "\n",
    "text = '垃圾分类，一般是指按一定规定或标准将垃圾分类储存、投放和搬运，从而转变成公共资源的一系列活动的总称。'\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(text))\n",
    "print(sp.encode_as_ids(text))\n",
    "\n",
    "print(sp.decode_pieces([['▁', '<0x0A>', '垃圾', '分类', ',', '一般', '是指', '按', '一定', '规定', '或', '标准', '将', '垃圾', '分类', '储存', '、', '投放', '和', '搬运', ',', '从而', '转变成', '公共', '资源', '的一系列', '活动', '的总称', '。', '<0x0A>']]))\n",
    "print(sp.decode_ids([sp.encode_as_ids(text)]))\n",
    "print(\"token_len:\",len(sp.encode_as_ids(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 18000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Before:32000\n",
      "New model pieces: 48724\n",
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf_48K\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Test text:\n",
      " 大模型是指具有非常大的参数数量的人工神经网络模型。 在深度学习领域，大模型通常是指具有数亿到数万亿参数的模型。\n",
      "Tokenized by LLaMA tokenizer:37,['▁', '垃圾', '分类', '，', '一般是', '指', '按', '一定', '规定', '或', '标准', '将', '垃圾', '分类', '储存', '、', '投', '放', '和', '搬', '运', '，', '从而', '转变', '成', '公共', '资源', '的一', '系列', '活动的', '总', '称', '。']\n",
      "Tokenized by YGPT-LLaMA tokenizer:37,['▁', '垃圾', '分类', '，', '一般是', '指', '按', '一定', '规定', '或', '标准', '将', '垃圾', '分类', '储存', '、', '投', '放', '和', '搬', '运', '，', '从而', '转变', '成', '公共', '资源', '的一', '系列', '活动的', '总', '称', '。']\n"
     ]
    }
   ],
   "source": [
    "## step4 merge_tokenizers\n",
    "### https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/c34ecaaafc5a77af6d04e588b2378b16dac95244/scripts/merge_tokenizer/merge_tokenizers.py#L4\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "## # load 两个分词器\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(\"./llama2_tokenzier_naive/\")  # 原生LLaMA分词模型\n",
    "chinese_sp_model = sentencepiece.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(\"./y-gpt-18k.model\")\n",
    "\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "# print number of tokens\n",
    "print(len(llama_tokenizer), len(chinese_sp_model))\n",
    "print(llama_tokenizer.all_special_tokens)\n",
    "print(llama_tokenizer.all_special_ids)\n",
    "print(llama_tokenizer.special_tokens_map)\n",
    "## Add Chinese tokens to LLaMA tokenizer\n",
    "llama_spm_tokens_set = set(p.piece for p in llama_spm.pieces)\n",
    "\n",
    "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)  # 将训练的分词模型追加新的token到之前的模型\n",
    "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
    "\n",
    "\n",
    "## Save\n",
    "output_sp_dir = \"merged_tokenizer_y_gpt\"\n",
    "model_file = \"y-gpt-18k.model\"\n",
    "output_hf_dir = f\"merged_tokenizer_hf_{48}K\"  # the path to save Chinese-LLaMA tokenizer\n",
    "os.makedirs(output_sp_dir, exist_ok=True)\n",
    "with open(output_sp_dir + f\"/{model_file}\", \"wb\") as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir + f\"/{model_file}\")\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n",
    "\n",
    "# Test\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "text1 = \"\"\"白日依山尽，黄河入海流。欲穷千里目，更上一层楼。\"\"\"\n",
    "text2 = \"\"\"大模型是指具有非常大的参数数量的人工神经网络模型。 在深度学习领域，大模型通常是指具有数亿到数万亿参数的模型。\"\"\"\n",
    "print(\"Test text:\\n\",text2)\n",
    "\n",
    "print(f\"Tokenized by LLaMA tokenizer:{len(llama_tokenizer.tokenize(text2))},{llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"Tokenized by YGPT-LLaMA tokenizer:{len(chinese_llama_tokenizer.tokenize(text2))},{chinese_llama_tokenizer.tokenize(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yyb-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
